```{r eval=F}
## paste this chunk into the ```{r setup} chunk at the top of your project 1 .Rmd file

knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```
---
title: 'Project 1: Exploratory Data Analysis on March Madness Predictions'
author: "Qinglong Yang qy929"
date: '4/4/2021'
output:
  html_document: 
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---




# Introduction
As an avid basketball fan and with March Madness fully underway, I wanted to do a project exploring predictors for success in the tournament. I found datasets from two different websites. The first dataset I found came from Kaggle and it featured many advanced statistics of college teams as well as how they fared in the NCAA tournament along with their tournament ranking. I found the second dataset on data.world and it features every NCAA team along with their BPI offensive, defensive, and overall rankings. BPI is a metric used by many sports predicting websites as a gauge of overall team strength. When filling out March Madness brackets BPI is shown for fans to help make picks and used by the selection committee to determine seeding for the teams. As a result, I found it interesting to see how accurately this metric will be able to predict team success in the tournament. There are also several variables inside the first dataset that I want to compare to team success include adjusted defensive efficiency and WAB, which is similar to BPI in that it looks at wins over a replacement team. However BPI defines a replacement team as a an average division one team whereas WAB defines it as an average bubble team. I expect to find a greater correlation with defensive BPI and defensive efficiency as defense has traditionally been a better predictor for me when I have filled out brackets. I hypothesize the reason for this being that good offensive teams are more reliant on shooting the ball well to win games. In big moments when nerves are higher, this dependency on shooting can work against teams and make them play worse on offense than they usually do, whereas defense is usually very consistent. Finally, while there were multiple years available in the datasets, I chose to use the 2017 year as that was the most recent tournament data provided in the datasets.

```{r}
library(tidyverse)
#Importing dataset 1
bpi <- read.csv("bpi.csv")
head(bpi)
#Importing dataset 2
team_data<-read.csv("cbb17.csv")
head(team_data)
```

# Dataset Join/Pivot
I chose to do a left join with BPI as my dataset that I want to keep. BPI contains many of the other variables I want to look into like defensive efficiency and WAB and compare. Joining the data this way also allowed me to eliminate all the teams that did not make the tournament leaving only the 68 teams that did (While the tournament itself is often seen as 64 teams, 68 teams technically make it as there are play ins for two 16 seeds and two 11 seeds).

```{r}
#Joining the datasets
total_data<-bpi%>%left_join(team_data,by=c("Team"="TEAM"))%>%na.omit
head(total_data)
#Showing only teams in tournament are in dataset
nrow(bpi)-(nrow(bpi)-nrow(total_data))

```


```{r}
#Wide pivot on the data frame
widepivot<- total_data %>% pivot_wider(names_from = POSTSEASON, values_from = POSTSEASON)  
head(widepivot)
#Long pivot on the data frame after the wide pivot
tidy<-widepivot%>%pivot_longer(c('Champions','E8','2ND','R32','S16','F4','R68','R64'),names_to="POSTSEASON",values_to="result",values_drop_na = T)%>%subset(select=-c(result))
head(tidy)
```

Because the data is already tidy and the join eliminated any need to tidy the data, the functions above are redundant and don't serve any real purpose. Each team has a unique position that they ended in the tournament and as a result there are no duplicates in the postseason finishes. As a result, when pivot longer was used a column had to be deleted as both of the columns would have the same values.

# Summary Statistics

This section demonstrates the 6 core dplyr functions. Data was first filtered based off the Big 12 (our conference) as I wanted to see how Big 12 teams performed in the tournament as well as some of their advanced stats. I then created a selected data dataset for use later on with only the relevant columns that I wanted to observe as the merged dataset came with several extra variables. Next, I used the help of the separate function to separate the string contained in W.L by the two numbers contained. The first number was assigned to a wins column and the second number was assigned to a losses column. Next, both wins and losses were made numeric from a string and then mutated to a new column called win_percentage to show win percentage by dividing wins by wins plus losses.

I then observed different summary statistics on wins, BPI, and WAB. Some low/mid major conferences only had one team make the tournament as they did not have any at large bids thus resulting in NAs for standard deviation and variance. Some interesting associations I found based off of the summary statistics includes that the WCC, despite having the highest mean BPI, only had the third highest mean WAB. This is likely due to strength of schedule as the WCC overall as a conference is much weaker than conferences like the Big 12 and ACC where every team is traditionally competitive to some degree. On the other hand, the WCC is very top loaded with Gonzaga, St. Mary's, and BYU being much stronger than the other teams. Here it can be demonstrated how WAB places much more emphasis on the level of competition than BPI does. Another interesting association that can be seen is that BPI ranked the Sun Belt team Troy with a positive BPI despite it having the second worst WAB. Looking into how Troy's actual game played out it can be seen that they played a very noncompetitive game against Duke. It seems that WAB is more accurate of a predictor. However this is only one data point so we will look towards graphs to provide a much more coherent picture. On the other hand, the team with the most wins was Gonzaga which had 37 wins on the season and the team with the least wins was South Dakota State with just 18.

```{r}
#Filtering data for big 12 teams and then selecting for certain variables of interest
big12<-total_data%>%filter(Conf=="Big 12")%>%arrange(BPI)
head(big12)
#Selecting for desired vairables
selected_data<-total_data%>%select(W.L,Team,BPI.Off,BPI.Def,BPI,ADJOE,ADJDE,WAB,POSTSEASON,Conf)
head(selected_data)
#Creating new variable win percentage
selected_data<-selected_data%>% separate(W.L,into=c("wins","losses"),remove=FALSE)%>%
transform(wins=as.numeric(wins),losses=as.numeric(losses))%>%mutate(win_percent=wins/(wins+losses))
head(selected_data)
#Table ofsummary statistics
selected_data%>%group_by(Conf)%>%summarise(mean_wins=mean(wins),sd_wins=sd(wins),var_wins=var(wins),mean_bpi=mean(BPI),sd_bpi=sd(BPI),var_bpi=var(BPI),mean_WAB=mean(WAB),sd_WAB=sd(WAB),var_WAB=var(WAB),max_wins=max(wins),min_wins=min(wins))

```


# Visualization
```{r}
#Creating a correlation matrix and assigning values for the categorical value postseason.
library(reshape2)
df<-total_data%>%select(POSTSEASON,BPI,WAB,BPI.Def,BPI.Off,ADJOE,ADJDE,BPI,SEED)
df<-df%>%mutate(POSTSEASON=recode(POSTSEASON, "Champions"=7,"2ND"=6,"F4"=5,"E8"=4,"S16"=3,"R32"=2,"R64"=1,"R68"=1))
head(df)
cormat<-cor(df)
abs_cormat<-abs(cormat)
#Displaying the correlation matrix, absolute value used as some variables are ordered in different directions
melted_cormat<-melt(abs_cormat)
ggplot(data=melted_cormat,aes(x=Var1,y=Var2,fill=value))+geom_tile()+scale_fill_gradient2(low="red",high="black",mid="white")

#Display scatterplot comparing adjusted offense to adjusted defensive rankings with respect to the team's finish in the tournament. Increased tick marks on scales than default
df%>%ggplot(aes(x=ADJOE,y=ADJDE,color=POSTSEASON))+
geom_point(size=3)+
ggtitle("Postseason Finish With Respect to Adjusted Offense and Adjusted Defense")+
xlab("Adjusted Offense")+
ylab("Adjusted Defense")+
  scale_x_continuous(breaks = round(seq(min(df$ADJOE), max(df$ADJOE), by = 2),1)) +
  scale_y_continuous(breaks = round(seq(min(df$ADJDE), max(df$ADJDE), by = 2),1))+
  theme_classic()

#Display barplot that includes stat=summary
df%>% ggplot(aes(x=SEED,y=POSTSEASON))+
geom_bar(stat="summary",fun.y="mean")+
geom_errorbar(stat="summary")+
  ggtitle("Postseason Finish With Respect to Seeding by the Committee")+
  theme_minimal()

```

The first graph created was a correlation heatmap that compared the correlation existing between each of the variables. This was done through first creating a data frame containing only the variables that I wanted to look for a correlation with using the select function. Because the tournament finish was a categorical value, mutate had to be used in order to assign a numerical value so that some correlation could be observed between postseason and the other variables. This was done with recode function with a higher number being assigned depending on how far the team made it in the tournament. Next, I used the melt function to create the correlation map after creating a matrix based off the correlations between the variables. Absolute value was utilized as certain variables like defensive efficiency and seed are ordered the other way (lower value means its better rather than higher). The round of 64 and round of 68 were given equal significance as neither progressed pass the opening day of the tournament. After looking at the correlation matrix, it is immediately apparent that the best predictors for tournament performance in 2017 came from overall seed and BPI, whereas adjusted offensive efficiency was the worst predictor. On the other hand, it's also apparent that offensive efficiency and defensive efficiency have a negative correlation with each other. It seems that teams that are good on offense have a tendency to be worse on defense and vice-versa. 

Next, I created a scatter plot that looked at adjusted offensive efficiency against adjusted defensive efficiency. This was done using ggplot. I also added more tick marks on both the x scale and the y scale. While the correlation matrix did show that in general, defensive efficiency has a better correlation than offensive correlation when it comes to March Madness results, the scatter plot shows an eye opening observation. It appears that the teams that did the best or made it the furthest in the tournament tended to be teams with a higher adjusted offense than defense. This shows that an elite level of offense tended to be slightly more important when it came to making a very deep run in the 2017 NCAA tournament. Thus, it can be said that for an average team, defense was more important, but for a team that made it especially far, the team tended to have an elite offense rather than defense. 

Lastly, I made a bar plot that utilized stat=summary in order to look at the effect of seeding on how far a team made it into the tournament. This plot shows an exponential relationship with higher seeds tending to make it significantly further into the tournament than a lower seed. In general, it shows that committee members were accurate with how they seeded teams in 2017 for the most part as higher seeded teams made it further than lower seeded teams.

# PCA
```{r}
#Running princomp on the variables in the dataframe
df_pca<-princomp(df)
summary(df_pca)
#Deciding number of PCAs to keep
eigval<-df_pca$sdev^2
varprop=round(eigval/sum(eigval),2)
ggplot()+geom_bar(aes(y=varprop,x=1:8),stat="identity")+xlab("")+geom_path(aes(y=varprop,x=1:8))+
geom_text(aes(x=1:8,y=varprop,label=round(varprop,2)),vjust=1,col="white",size=5)+
scale_y_continuous(breaks=seq(0,.6,.2),labels = scales::percent)+
scale_x_continuous(breaks=1:10)
#Results summary
summary(df_pca, loadings=T)
#Showing datapoints of the dataframe with respect to PC1 and PC2
ggplot()+geom_point(aes(df_pca$scores[,1], df_pca$scores[,2]))+xlab("PC1")+ylab("PC2")
library(factoextra)
#Showing correlation of variables with respect to the Principle Components
fviz_pca_var(df_pca,col.var="contrib",gradient.cols=c("red","orange","yellow"),repel=TRUE)

```


I decided on the use of just two Principal components as they account for 96 percent of the variance present in the data. It seems that for principal component one, the bulk of the variance is based off of the variables BPI, WAB, and Seed. This makes sense considering the fact that BPI and WAB are very similar metrics, the only difference being that BPI bases a replacement team as an average D1 team while WAB uses an average bubble team. Also, BPI and WAB are used by the selection committee when deciding on seeding. In theory, these 3 variables should determine how far a team makes it in the tournament barring an upset. However upsets are inevitable as with any sporting event. This 80 percent value in a way shows how accurate the selection committee is as in theory seeding, BPI, and WAB should are modeled to predict how good teams are and would account for all variability in a world where upsets did not happen. On the other hand, PC2 seems to have a mix of most of the other variables other than BPI, WAB, and Seed. PC2 accounts for significantly less variance at 15.1 percent than PC1 at 80.9 percent and does not have any single variable that appears to be significantly affecting it. 








